{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06983019-5c26-4d95-9dc8-b5b2e7aec138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import argparse\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import pyttsx3\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1312f50e-efe9-4b43-810b-aa685f9656fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(x_center, frame_width):\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the horizontal direction of the detected object.\n",
    "    Splits the frame into 5 zones: Far Left, Slightly Left, Center, Slightly Right, Far Right.\n",
    "    \"\"\"\n",
    "    \n",
    "    relative_pos = float(x_center / frame_width)\n",
    "\n",
    "    print(frame_width ,x_center)\n",
    "    if relative_pos < 0.2:\n",
    "        return \"far left\"\n",
    "    elif relative_pos < 0.4:\n",
    "        return \"slightly left\"\n",
    "    elif relative_pos < 0.6:\n",
    "        return \"center\"\n",
    "    elif relative_pos < 0.8:\n",
    "        return \"slightly right\"\n",
    "    else:\n",
    "        return \"far right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1a6cbe-c7d2-4017-be4f-85ccf6fe16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_distance(pixel_width: float, known_width: float, focal_length: float) -> float:\n",
    "    \"\"\"Estimate distance from camera in cm.\"\"\"\n",
    "    if focal_length is None:\n",
    "        return -1\n",
    "    return (known_width * focal_length) / pixel_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a644cdf-45d0-48b2-92f7-ed3486a92402",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOCAL_LENGTH = 707.94 \n",
    "KNOWN_WIDTHS = {\n",
    "    \"person\": 0.5,\n",
    "    \"cup\": 0.08,\n",
    "    \"chair\": 0.45,\n",
    "    \"laptop\": 0.30,\n",
    "    \"bottle\": 0.07,\n",
    "}\n",
    "\n",
    "def draw_boxes(image_bgr, results, score_threshold=0.5):\n",
    "    out = image_bgr.copy()\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            if conf < score_threshold:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            cls = r.names[cls_id]\n",
    "            cv2.rectangle(out, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            text = f\"{cls}: {conf:.2f}\"\n",
    "            ((text_w, text_h), _) = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(out, (x1, y1 - text_h - 6), (x1 + text_w, y1), (0, 255, 0), -1)\n",
    "            cv2.putText(out, text, (x1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    return out\n",
    "\n",
    "def run_image_mode(input_path: str, score: float = 0.5, output_path: str = None, weights: str = \"YOLOv10x.pt\"):\n",
    "    model = YOLO(weights)\n",
    "    img = cv2.imread(input_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: could not read {input_path}\")\n",
    "        return\n",
    "    results = model(img)\n",
    "    drawn = draw_boxes(img, results, score)\n",
    "    if output_path:\n",
    "        cv2.imwrite(output_path, drawn)\n",
    "        print(f\"Wrote annotated image to {output_path}\")\n",
    "    else:\n",
    "        names = extract_names(img ,results, score)\n",
    "        tts(names)\n",
    "        cv2.imshow('detected', drawn)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def run_webcam_mode(score: float = 0.5, frame_resize: float = 0.6, frame_skip: int = 3, weights: str = \"YOLOv10x.pt\"):\n",
    "    model = YOLO(weights)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"ERROR: Could not open webcam\")\n",
    "        return\n",
    "    engine = None\n",
    "    last_spoken = []\n",
    "    frame_idx = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_idx += 1\n",
    "            if frame_idx % frame_skip != 0:\n",
    "                cv2.imshow('webcam', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                continue\n",
    "            if frame_resize != 1.0:\n",
    "                small = cv2.resize(frame, (0, 0), fx=frame_resize, fy=frame_resize)\n",
    "            else:\n",
    "                small = frame\n",
    "            results = model(small)\n",
    "            out_small = draw_boxes(small, results, score)\n",
    "            names = extract_names(small , results, score)\n",
    "\n",
    "            \n",
    "            if names and names != last_spoken:\n",
    "                tts(names, engine)\n",
    "                last_spoken = names\n",
    "            cv2.imshow('webcam', out_small)\n",
    "            cv2.waitKey(0)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted by user')\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "def tts(names: List[str], engine=None):\n",
    "    if not names:\n",
    "        return\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for n in names:\n",
    "        if n not in seen:\n",
    "            seen.add(n)\n",
    "            unique.append(n)\n",
    "    sentence = \"I see \" + (\" and \".join(unique))\n",
    "    try:\n",
    "        if engine is None:\n",
    "            engine = pyttsx3.init()\n",
    "            engine.setProperty('rate', 150)\n",
    "            voices = engine.getProperty('voices')\n",
    "            if len(voices) > 1:\n",
    "                engine.setProperty('voice', voices[1].id)\n",
    "        engine.say(sentence)\n",
    "        engine.runAndWait()\n",
    "    except Exception as e:\n",
    "        print(\"TTS error:\", e)\n",
    "        \n",
    "def extract_names(image_bgr, results, score_threshold=0.5):\n",
    "    names = []\n",
    "    out = image_bgr.copy()\n",
    "    \n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            conf = float(box.conf[0])\n",
    "            cls_id = int(box.cls[0])\n",
    "            label = r.names[cls_id]\n",
    "            \n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            x_center = (x1 + x2) // 2\n",
    "\n",
    "            if conf < score_threshold:\n",
    "                continue\n",
    "            cls_id = int(box.cls[0])\n",
    "            distance = 0\n",
    "            if label in KNOWN_WIDTHS:\n",
    "                distance = estimate_distance(w , KNOWN_WIDTHS[label] , FOCAL_LENGTH)\n",
    "\n",
    "            direction = get_direction(x_center , out.shape[1])\n",
    "            if distance > 0:\n",
    "                names.append(f\"{r.names[cls_id]} to {direction}, about {distance:.1f} meters away\")\n",
    "            else:\n",
    "                names.append(f\"{r.names[cls_id]} to {direction}\")\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5da1272d-341c-4c03-93bf-bf91b3c050d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 816.4ms\n",
      "Speed: 3.7ms preprocess, 816.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "640 244\n",
      "\n",
      "0: 480x640 1 person, 888.8ms\n",
      "Speed: 3.4ms preprocess, 888.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "640 295\n",
      "\n",
      "0: 480x640 1 person, 851.1ms\n",
      "Speed: 2.2ms preprocess, 851.1ms inference, 0.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "640 249\n",
      "\n",
      "0: 480x640 1 person, 775.6ms\n",
      "Speed: 2.1ms preprocess, 775.6ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "640 270\n",
      "\n",
      "0: 480x640 1 person, 845.7ms\n",
      "Speed: 2.7ms preprocess, 845.7ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "640 266\n",
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "run_webcam_mode(frame_skip = 1 , frame_resize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e70e106f-78f1-4757-aa6c-631cdcc10813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 5 chairs, 1 dining table, 726.0ms\n",
      "Speed: 4.5ms preprocess, 726.0ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "1024 648\n",
      "1024 708\n",
      "1024 424\n",
      "1024 732\n",
      "1024 436\n",
      "1024 549\n"
     ]
    }
   ],
   "source": [
    "run_image_mode(\"./images/train/images/00d343d4a829121c.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43498657-20c2-454a-a95d-efee49128620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' when done selecting ROI...\n",
      "Estimated focal length: 707.94 pixels\n"
     ]
    }
   ],
   "source": [
    "# Real world width of your calibration object (meters)\n",
    "KNOWN_WIDTH = 0.0856  # credit card width\n",
    "# Distance from camera to object during calibration (meters)\n",
    "KNOWN_DISTANCE = 0.3  \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 'q' when done selecting ROI...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Calibration - place object at known distance\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # Select ROI (draw bounding box around object)\n",
    "        roi = cv2.selectROI(\"Calibration\", frame, False, False)\n",
    "        x, y, w, h = roi\n",
    "        pixel_width = w\n",
    "        focal_length = (pixel_width * KNOWN_DISTANCE) / KNOWN_WIDTH\n",
    "        print(f\"Estimated focal length: {focal_length:.2f} pixels\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fba09-8c58-44be-90d4-df578b5fc38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HiddenEyekernel",
   "language": "python",
   "name": "hiddeneyekernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
